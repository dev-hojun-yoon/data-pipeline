version: '3.8'

networks:
  hadoop-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  zookeeper-1-data:
  zookeeper-1-logs:
  kafka-1-data:
  mongodb-1-data:
  mongodb-1-config:
  spark-master-logs:

services:
  # Zookeeper 클러스터 - Node 1 (Single node for the cluster)
  zookeeper-1:
    image: confluentinc/cp-zookeeper:7.4.0
    hostname: zookeeper-1
    container_name: zookeeper-1
    ports:
      - "2189:2189"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2189
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888
    volumes:
      - zookeeper-1-data:/var/lib/zookeeper/data
      - zookeeper-1-logs:/var/lib/zookeeper/log
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.10
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2189"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Kafka 브로커 1
  kafka-1:
    image: confluentinc/cp-kafka:7.4.0
    hostname: kafka-1
    container_name: kafka-1
    depends_on:
      zookeeper-1:
        condition: service_healthy
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: '192.168.0.12:2189'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: EXTERNAL://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: EXTERNAL://192.168.0.12:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: EXTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 2
      KAFKA_MIN_IN_SYNC_REPLICAS: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
    volumes:
      - kafka-1-data:/var/lib/kafka/data
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.11
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Kafka UI
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka-1
    ports:
      - "3030:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: hadoop-kafka-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: 192.168.0.12:9092,192.168.0.13:9093
      KAFKA_CLUSTERS_0_ZOOKEEPER: '192.168.0.12:2189'
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.12
    restart: unless-stopped

  # MongoDB Primary
  mongodb-1:
    image: mongo:7.0
    hostname: mongodb-1
    container_name: mongodb-1
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: password123
      MONGO_INITDB_DATABASE: financial_db
    volumes:
      - mongodb-1-data:/data/db
      - mongodb-1-config:/data/configdb
      - ./scripts/mongo-keyfile:/etc/mongo/mongo-keyfile
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.13
    command:
      - /bin/bash
      - -c
      - |
        # 키파일 권한 설정
        if [ -f /etc/mongo/mongo-keyfile ]; then
          chmod 400 /etc/mongo/mongo-keyfile
          chown mongodb:mongodb /etc/mongo/mongo-keyfile
        fi
        
        # 첫 실행인지 확인
        if [ ! -f /data/db/.mongodb_initialized ]; then
          echo 'First time setup - starting without authentication' 
          mongod --bind_ip_all --port 27017 & 
          MONGO_PID=$${!} 
          
          # MongoDB 준비 대기
          echo 'Waiting for MongoDB to start...' 
          until mongosh --port 27017 --eval 'db.runCommand({ping: 1})' > /dev/null 2>&1; do
            sleep 2
          done
          
          echo 'Creating admin user...' 
          mongosh --port 27017 --eval "db.getSiblingDB('admin').createUser({user: '$${MONGO_INITDB_ROOT_USERNAME}', pwd: '$${MONGO_INITDB_ROOT_PASSWORD}', roles: [{role: 'root', db: 'admin'}]})"
          
          echo 'Creating application database and user...' 
          mongosh --port 27017 --eval "db.getSiblingDB('$${MONGO_INITDB_DATABASE}').createUser({user: 'app_user', pwd: 'app_password', roles: [{role: 'readWrite', db: '$${MONGO_INITDB_DATABASE}'}]})"

          echo 'Forcing database creation by creating a placeholder collection...' 
          mongosh --port 27017 --eval "db.getSiblingDB('$${MONGO_INITDB_DATABASE}').createCollection('placeholder')"
          
          # 초기화 완료 표시
          touch /data/db/.mongodb_initialized
          
          # MongoDB 종료 (인증 사용)
          echo 'Shutting down MongoDB with authentication...' 
          mongosh --port 27017 -u '$${MONGO_INITDB_ROOT_USERNAME}' -p '$${MONGO_INITDB_ROOT_PASSWORD}' --authenticationDatabase 'admin' --eval 'db.getSiblingDB("admin").shutdownServer()' || true
          wait $${MONGO_PID}
        fi
        
        echo 'Starting MongoDB with replica set (waiting for other nodes)...' 
        # 레플리카셋 모드로 시작하되, 초기화는 나중에
        if [ -f /etc/mongo/mongo-keyfile ]; then
          exec mongod --replSet rs0 --bind_ip_all --auth --keyFile /etc/mongo/mongo-keyfile
        else
          exec mongod --replSet rs0 --bind_ip_all --auth
        fi
    healthcheck:
      test: ["CMD", "mongosh", "--port", "27017", "--eval", "db.adminCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # Spark Master
  spark-master:
    image: bitnami/spark:3.5
    hostname: spark-master
    container_name: spark-master
    ports:
      - "8081:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    volumes:
      - spark-master-logs:/opt/bitnami/spark/logs
      - ./spark-jobs:/opt/spark-jobs
      - ./spark-libs:/opt/spark-libs
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.14
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped


  # Spark Job Submitter - HDFS에서 데이터 읽기 및 Kafka 전송
    spark-submit-job:
    image: bitnami/spark:3.5
    container_name: spark-submit-job
    depends_on:
      - spark-master
      - kafka-1
    environment:
      - SPARK_MODE=submit
      - SPARK_MASTER_URL=spark://spark-master:7077
      - HADOOP_USER_NAME=hadoop
      - HADOOP_CONF_DIR=/etc/hadoop
      - SPARK_SUBMIT_ARGS=--class DataPipelineJob --master spark://spark-master:7077 --deploy-mode client
      # HDFS 접근을 위한 환경 변수
      - HDFS_NAMENODE_USER=hdfs
      - HDFS_DATANODE_USER=hdfs
      - HDFS_SECONDARYNAMENODE_USER=hdfs
    volumes:
      - ./spark_producer.py:/app/spark_producer.py
      - ./spark-libs:/opt/spark-libs
      # Hadoop 설정 파일들을 마운트
      - ./hadoop-conf/core-site.xml:/etc/hadoop/core-site.xml
      - ./hadoop-conf/hdfs-site.xml:/etc/hadoop/hdfs-site.xml
      - ./hadoop-conf/yarn-site.xml:/etc/hadoop/yarn-site.xml
      # 테스트 스크립트 마운트
      - ./scripts/hdfs-test.sh:/app/hdfs-test.sh
      # 추가적인 Hadoop 라이브러리
      - ./hadoop-libs:/opt/hadoop-libs
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.15
    # HDFS 클러스터의 모든 노드에 대한 호스트 매핑
    extra_hosts:
      - "namenode:192.168.0.9"           # 네임노드 IP
      - "datanode3:192.168.0.12"        # 데이터노드3 IP (현재 kafka가 있는 노드)
      - "datanode4:192.168.0.13"        # 데이터노드4 IP
    command: >
      bash -c "
        echo 'Spark HDFS Producer 시작...' &&

        # 필요한 패키지 설치 (네트워크 도구)
        apt-get update -qq && apt-get install -y -qq netcat-traditional telnet curl > /dev/null 2>&1 &&

        # 기본 대기 시간
        echo 'Spark 마스터 준비 대기 중...' &&
        sleep 30 &&

        # 연결 테스트 함수 정의
        test_connection() {
          local host=$$1
          local port=$$2
          local service=$$3
          echo \"테스트 중: $$service ($$host:$$port)\" 

          # timeout 명령어와 함께 사용 (10초 타임아웃)
          if timeout 10 bash -c \"cat < /dev/null > /dev/tcp/$$host/$$port\" 2>/dev/null; then
            echo \"✓ $$service 연결 성공\" 
            return 0
          else
            echo \"✗ $$service 연결 실패\" 
            return 1
          fi
        } &&

        # HDFS 네임노드 연결 테스트
        echo '=== HDFS 연결 테스트 시작 ===' &&
        max_attempts=30 &&
        attempt=0 &&

        while [ $$attempt -lt $$max_attempts ]; do
          if test_connection namenode 9000 'HDFS NameNode'; then
            echo 'HDFS 네임노드 연결 확인됨!'
            break
          fi

          attempt=$$(($$attempt + 1))
          echo \"연결 재시도 $$attempt/$$max_attempts...\" 
          sleep 10
        done &&

        if [ $$attempt -ge $$max_attempts ]; then
          echo '경고: HDFS 네임노드 연결 실패, 하지만 계속 진행...' 
        fi &&

        # Hadoop 설정 확인
        echo '=== Hadoop 설정 확인 ===' &&
        if [ -f '/etc/hadoop/core-site.xml' ]; then
          echo '✓ core-site.xml 파일 존재' 
          cat /etc/hadoop/core-site.xml | grep -A1 'fs.defaultFS' || echo '  fs.defaultFS 설정 확인 불가' 
        else
          echo '✗ core-site.xml 파일 없음' 
        fi &&

        # HDFS 파일시스템 테스트
        echo '=== HDFS 파일시스템 테스트 ===' &&
        if timeout 30 hadoop fs -ls / > /tmp/hdfs_test.out 2>&1; then
          echo '✓ HDFS 루트 디렉토리 접근 성공' 
          echo '루트 디렉토리 내용:' 
          cat /tmp/hdfs_test.out
        else
          echo '✗ HDFS 루트 디렉토리 접근 실패' 
          echo '오류 내용:' 
          cat /tmp/hdfs_test.out 2>/dev/null || echo '오류 로그 없음'
        fi &&

        # 타겟 파일 존재 확인
        echo '=== 타겟 파일 확인 ===' &&
        target_file='/user/hadoop3/test.json' &&
        if timeout 30 hadoop fs -test -e $$target_file; then
          echo \"✓ 파일 존재: $$target_file\" 
          echo '파일 정보:' 
          timeout 15 hadoop fs -ls $$target_file || echo '파일 정보 조회 실패' 
          echo '파일 크기:' 
          timeout 15 hadoop fs -du -h $$target_file || echo '파일 크기 조회 실패' 
        else
          echo \"✗ 파일 없음: $$target_file\" 
          echo '상위 디렉토리 확인:' 
          timeout 15 hadoop fs -ls /user/hadoop3/ 2>/dev/null || echo '/user/hadoop3/ 디렉토리 접근 불가' 
          timeout 15 hadoop fs -ls /user/ 2>/dev/null || echo '/user/ 디렉토리 접근 불가'
        fi &&

        # Kafka 연결 테스트
        echo '=== Kafka 브로커 테스트 ===' &&
        test_connection 192.168.0.12 9092 'Kafka Broker 1' || echo '  Kafka Broker 1 연결 실패, 계속 진행...' &&
        test_connection 192.168.0.13 9093 'Kafka Broker 2' || echo '  Kafka Broker 2 연결 실패, 계속 진행...' &&

        # Spark 작업 실행
        echo '=== Spark 작업 시작 ===' &&
        chmod +x /app/spark_producer.py &&

        # Spark 실행 환경 정보
        echo \"Spark 홈: $$SPARK_HOME\" &&
        echo \"Java 홈: $$JAVA_HOME\" &&
        echo \"Hadoop 설정 디렉토리: $$HADOOP_CONF_DIR\" &&

        # 실제 Spark 작업 실행
        spark-submit \
          --master spark://spark-master:7077 \
          --deploy-mode client \
          --driver-memory 2g \
          --executor-memory 2g \
          --executor-cores 2 \
          --num-executors 2 \
          --jars /opt/spark-libs/kafka-clients-3.5.0.jar,/opt/spark-libs/spark-sql-kafka-0-10_2.12-3.5.0.jar \
          --files /etc/hadoop/core-site.xml,/etc/hadoop/hdfs-site.xml \
          --conf spark.driver.extraJavaOptions='-Dlog4j.configuration=file:/opt/bitnami/spark/conf/log4j.properties' \
          --conf spark.executor.extraJavaOptions='-Dlog4j.configuration=file:/opt/bitnami/spark/conf/log4j.properties' \
          --conf spark.hadoop.fs.defaultFS=hdfs://namenode:9000 \
          --conf spark.hadoop.dfs.client.use.datanode.hostname=false \
          --conf spark.hadoop.dfs.datanode.use.datanode.hostname=false \
          --conf spark.sql.adaptive.enabled=true \
          --conf spark.sql.adaptive.coalescePartitions.enabled=true \
          --conf spark.network.timeout=300s \
          --conf spark.sql.execution.arrow.pyspark.enabled=false \
          /app/spark_producer.py \
          --json-file /user/hadoop3/test.json \
          --kafka-brokers 192.168.0.12:9092,192.168.0.13:9093 || \
        (echo 'Spark 작업 실패, 로그 확인 중...' && 
          find /opt/bitnami/spark/logs -name '*.log' -exec echo 'Log file: {}' \; -exec tail -50 {} \; 2>/dev/null || 
          echo '로그 파일을 찾을 수 없음')
      "
    restart: unless-stopped


  # Kafka to MongoDB Consumer
  kafka-mongo-consumer:
    build:
      context: .
      dockerfile: Dockerfile.consumer
    container_name: kafka-mongo-consumer
    depends_on:
      - kafka-1
      - mongodb-1
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=192.168.0.12:9092,192.168.0.13:9093
      - MONGO_HOST=mongodb
      - MONGO_PORT=27017
      - MONGO_DB=financial_db
      - MONGO_USER=admin
      - MONGO_PASSWORD=admin123
    volumes:
      - ./consumer-app:/app
    working_dir: /app
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.16
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "pgrep", "-f", "KafkaMongoConsumer"]
      interval: 30s
      timeout: 10s
      retries: 3

  # MongoDB 클러스터 초기화 서비스 (일회성)
  mongodb-cluster-init:
    image: mongo:7.0
    container_name: mongodb-cluster-init
    depends_on:
      - mongodb-1
    environment:
      - MONGO_INITDB_ROOT_USERNAME=admin
      - MONGO_INITDB_ROOT_PASSWORD=password123
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.17
    command:
      - /bin/bash
      - -c
      - |
        echo 'Waiting for all MongoDB instances to be ready...' 
        sleep 120  # DN4 MongoDB들이 시작될 시간을 충분히 대기
        
        # Primary 연결 시도
        echo 'Connecting to MongoDB Primary...' 
        until mongosh --host 192.168.0.12:27017 -u admin -p password123 --authenticationDatabase admin --eval "db.runCommand({ping: 1})" > /dev/null 2>&1; do
          echo 'Waiting for Primary MongoDB...' 
          sleep 10
        done
        
        # Secondary 연결 확인
        echo 'Checking Secondary MongoDB...' 
        until nc -z 192.168.0.13 27018; do
          echo 'Waiting for Secondary MongoDB...' 
          sleep 10
        done
        
        # Arbiter 연결 확인  
        echo 'Checking Arbiter MongoDB...' 
        until nc -z 192.168.0.13 27019; do
          echo 'Waiting for Arbiter MongoDB...' 
          sleep 10
        done
        
        echo 'All MongoDB instances are ready. Initializing replica set...' 
        mongosh --host 192.168.0.12:27017 -u admin -p password123 --authenticationDatabase admin --eval "
          try {
            rs.initiate({
              _id: 'rs0',
              members: [
                { _id: 0, host: '192.168.0.12:27017', priority: 2 },
                { _id: 1, host: '192.168.0.13:27018', priority: 1 },
                { _id: 2, host: '192.168.0.13:27019', arbiterOnly: true }
              ]
            });
            print('Replica set initialized successfully');
          } catch (error) {
            if (error.code === 23) {
              print('Replica set already initialized');
            } else {
              print('Error initializing replica set: ' + error);
            }
          }
        "
        
        echo 'Waiting for primary election...' 
        sleep 30
        
        # 레플리카셋 상태 확인
        mongosh --host 192.168.0.12:27017 -u admin -p password123 --authenticationDatabase admin --eval "
          print('Replica set status:');
          printjson(rs.status());
        "
        
        echo 'MongoDB cluster initialization completed'
    restart: "no"  # 일회성 실행