
version: '3.8'

networks:
  hadoop-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  zookeeper-1-data:
  zookeeper-1-logs:
  kafka-1-data:
  mongodb-1-data:
  mongodb-1-config:
  spark-master-logs:
  airflow-postgres-data:

services:
  # Zookeeper 클러스터 - Node 1 (Single node for the cluster)
  zookeeper-1:
    image: confluentinc/cp-zookeeper:7.4.0
    hostname: zookeeper-1
    container_name: zookeeper-1
    ports:
      - "2189:2189"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2189
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-1-data:/var/lib/zookeeper/data
      - zookeeper-1-logs:/var/lib/zookeeper/log
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.10
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2189"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Kafka 브로커 1
  kafka-1:
    image: confluentinc/cp-kafka:7.4.0
    hostname: kafka-1
    container_name: kafka-1
    depends_on:
      zookeeper-1:
        condition: service_healthy
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: '172.20.1.10:2189'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: EXTERNAL://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: EXTERNAL://192.168.0.12:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: EXTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 2
      KAFKA_MIN_IN_SYNC_REPLICAS: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
    volumes:
      - kafka-1-data:/var/lib/kafka/data
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.11
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Kafka UI
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka-1
    ports:
      - "3030:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: hadoop-kafka-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: 192.168.0.12:9092,192.168.0.13:9093
      KAFKA_CLUSTERS_0_ZOOKEEPER: 'zookeeper-1:2189'
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.12
    restart: unless-stopped

  # MongoDB Primary
  mongodb-1:
    image: mongo:7.0
    hostname: mongodb-1
    container_name: mongodb-1
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: password123
      MONGO_INITDB_DATABASE: financial_db
    volumes:
      - mongodb-1-data:/data/db
      - mongodb-1-config:/data/configdb
      - ./scripts/mongo-keyfile:/etc/mongo/mongo-keyfile
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.13
    command:
      - /bin/bash
      - -c
      - |
        if [ -f /etc/mongo/mongo-keyfile ]; then chmod 400 /etc/mongo/mongo-keyfile && chown mongodb:mongodb /etc/mongo/mongo-keyfile; fi
        if [ ! -f /data/db/.mongodb_initialized ]; then
          echo 'First time setup - starting without authentication'
          mongod --bind_ip_all --port 27017 & MONGO_PID=$${!}
          until mongosh --port 27017 --eval 'db.runCommand({ping: 1})' > /dev/null 2>&1; do sleep 2; done
          echo 'Creating admin user...'
          mongosh --port 27017 --eval "db.getSiblingDB('admin').createUser({user: '$${MONGO_INITDB_ROOT_USERNAME}', pwd: '$${MONGO_INITDB_ROOT_PASSWORD}', roles: [{role: 'root', db: 'admin'}]})"
          echo 'Creating application database and user...'
          mongosh --port 27017 --eval "db.getSiblingDB('$${MONGO_INITDB_DATABASE}').createUser({user: 'app_user', pwd: 'app_password', roles: [{role: 'readWrite', db: '$${MONGO_INITDB_DATABASE}'}]})"
          echo 'Forcing database creation by creating a placeholder collection...'
          mongosh --port 27017 --eval "db.getSiblingDB('$${MONGO_INITDB_DATABASE}').createCollection('placeholder')"
          touch /data/db/.mongodb_initialized
          echo 'Shutting down MongoDB with authentication...'
          mongosh --port 27017 -u '$${MONGO_INITDB_ROOT_USERNAME}' -p '$${MONGO_INITDB_ROOT_PASSWORD}' --authenticationDatabase 'admin' --eval 'db.getSiblingDB("admin").shutdownServer()' || true
          wait $${MONGO_PID}
        fi
        echo 'Starting MongoDB with replica set (waiting for other nodes)...'
        if [ -f /etc/mongo/mongo-keyfile ]; then exec mongod --replSet rs0 --bind_ip_all --auth --keyFile /etc/mongo/mongo-keyfile; else exec mongod --replSet rs0 --bind_ip_all --auth; fi
    healthcheck:
      test: ["CMD", "mongosh", "--port", "27017", "--eval", "db.adminCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # Spark Master
  spark-master:
    image: bitnami/spark:3.5
    hostname: spark-master
    container_name: spark-master
    ports:
      - "8081:8081"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
    command: >
      /opt/bitnami/spark/bin/spark-class -Dspark.public.dns=192.168.0.12 org.apache.spark.deploy.master.Master
      --host 0.0.0.0 --port 7077 --webui-port 8081
    volumes:
      - spark-master-logs:/opt/bitnami/spark/logs
      - ./spark-jobs:/opt/spark-jobs
      - ./spark-libs:/opt/spark-libs
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.14
    extra_hosts:
      - "namenode:192.168.0.9"
      - "datanode1:192.168.0.10"
      - "datanode2:192.168.0.11"
      - "datanode3:192.168.0.12"
      - "datanode4:192.168.0.13"
      - "kafka-2:192.168.0.13"
      - "spark-worker:192.168.0.13"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Kafka to MongoDB Consumer
  kafka-mongo-consumer:
    build:
      context: ..
      dockerfile: Dockerfile.consumer
    container_name: kafka-mongo-consumer
    depends_on:
      - kafka-1
      - mongodb-1
    environment:
      - KAFKA_HOSTS=192.168.0.12:9092,192.168.0.13:9093
      - MONGO_HOST=mongodb-1
      - MONGO_PORT=27017
      - MONGO_DB=financial_db
      - MONGO_USER=admin
      - MONGO_PASSWORD=password123
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.16
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "pgrep", "-f", "KafkaMongoConsumer"]
      interval: 30s
      timeout: 10s
      retries: 3

  # MongoDB 클러스터 초기화 서비스 (일회성)
  mongodb-cluster-init:
    image: mongo:7.0
    container_name: mongodb-cluster-init
    depends_on:
      - mongodb-1
    environment:
      - MONGO_INITDB_ROOT_USERNAME=admin
      - MONGO_INITDB_ROOT_PASSWORD=password123
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.17
    command:
      - /bin/bash
      - -c
      - |
        echo 'Waiting for all MongoDB instances to be ready...'
        sleep 120
        echo 'Connecting to MongoDB Primary...'
        until mongosh --host 192.168.0.12:27017 -u admin -p password123 --authenticationDatabase admin --eval "db.runCommand({ping: 1})" > /dev/null 2>&1; do echo 'Waiting for Primary MongoDB...'; sleep 10; done
        echo 'Checking Secondary MongoDB...'
        until nc -z 192.168.0.13 27018; do echo 'Waiting for Secondary MongoDB...'; sleep 10; done
        echo 'Checking Arbiter MongoDB...'
        until nc -z 192.168.0.13 27019; do echo 'Waiting for Arbiter MongoDB...'; sleep 10; done
        echo 'All MongoDB instances are ready. Initializing replica set...'
        mongosh --host 192.168.0.12:27017 -u admin -p password123 --authenticationDatabase admin --eval "try { rs.initiate({ _id: 'rs0', members: [ { _id: 0, host: '192.168.0.12:27017', priority: 2 }, { _id: 1, host: '192.168.0.13:27018', priority: 1 }, { _id: 2, host: '192.168.0.13:27019', arbiterOnly: true } ] }); print('Replica set initialized successfully'); } catch (error) { if (error.code === 23) { print('Replica set already initialized'); } else { print('Error initializing replica set: ' + error); } }"
        echo 'Waiting for primary election...'
        sleep 30
        mongosh --host 192.168.0.12:27017 -u admin -p password123 --authenticationDatabase admin --eval " print('Replica set status:'); printjson(rs.status()); "
        echo 'MongoDB cluster initialization completed'
    restart: "no"

  # --- Airflow Services ---
  airflow-postgres:
    image: postgres:13
    container_name: airflow-postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - airflow-postgres-data:/var/lib/postgresql/data
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.20
    restart: unless-stopped

  airflow-redis:
    image: redis:latest
    container_name: airflow-redis
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.21
    restart: unless-stopped

  airflow-init:
    image: apache/airflow:2.8.1
    container_name: airflow-init
    env_file:
      - ../airflow/.env
    depends_on:
      - airflow-postgres
      - airflow-redis
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      - AIRFLOW__CELERY__BROKER_URL=redis://airflow-redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@airflow-postgres/airflow
    command: ["bash", "-c", "airflow db init && airflow users create --username admin --password admin --firstname Anonymous --lastname User --role Admin --email admin@example.org"]
    networks:
      - hadoop-network
    restart: on-failure

  airflow-webserver:
    image: apache/airflow:2.8.1
    container_name: airflow-webserver
    env_file:
      - ../airflow/.env
    depends_on:
      - airflow-init
    ports:
      - "8080:8080"
    volumes:
      - ../:/opt/airflow/project
      - ../airflow/dags:/opt/airflow/dags
      - ../airflow/logs:/opt/airflow/logs
      - ../airflow/plugins:/opt/airflow/plugins
      - ../airflow/requirements.txt:/requirements.txt
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.22
    command: ["bash", "-c", "pip install --no-cache-dir -r /requirements.txt && airflow webserver"]
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  airflow-scheduler:
    image: apache/airflow:2.8.1
    container_name: airflow-scheduler
    env_file:
      - ../airflow/.env
    depends_on:
      - airflow-webserver
    volumes:
      - ../:/opt/airflow/project
      - ../airflow/dags:/opt/airflow/dags
      - ../airflow/logs:/opt/airflow/logs
      - ../airflow/plugins:/opt/airflow/plugins
      - ../airflow/requirements.txt:/requirements.txt
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.23
    command: ["bash", "-c", "pip install --no-cache-dir -r /requirements.txt && airflow scheduler"]
    extra_hosts:
      - "namenode:192.168.0.9"
      - "datanode1:192.168.0.10"
      - "datanode2:192.168.0.11"
      - "datanode3:192.168.0.12"
      - "datanode4:192.168.0.13"
      - "kafka-2:192.168.0.13"
      - "spark-worker:192.168.0.13"
    restart: unless-stopped

  airflow-worker:
    image: apache/airflow:2.8.1
    container_name: airflow-worker
    env_file:
      - ../airflow/.env
    depends_on:
      - airflow-scheduler
    volumes:
      - ../:/opt/airflow/project
      - ../airflow/dags:/opt/airflow/dags
      - ../airflow/logs:/opt/airflow/logs
      - ../airflow/plugins:/opt/airflow/plugins
      - ../airflow/requirements.txt:/requirements.txt
    networks:
      hadoop-network:
        ipv4_address: 172.20.1.24
    command: ["bash", "-c", "pip install --no-cache-dir -r /requirements.txt && airflow celery worker"]
    extra_hosts:
      - "namenode:192.168.0.9"
      - "datanode1:192.168.0.10"
      - "datanode2:192.168.0.11"
      - "datanode3:192.168.0.12"
      - "datanode4:192.168.0.13"
      - "kafka-2:192.168.0.13"
      - "spark-worker:192.168.0.13"
    restart: unless-stopped
